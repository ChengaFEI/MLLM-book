title: Why do LLaVA Vision-Language Models Reply to Images in English?

key_work: MLLM, VLM

date of publish: 2024.07

Core idea:
    1. abstract: This paper investigates a significant multilingual bias in LLaVA-style vision-language models (VLMs), revealing that including an image in a query substantially increases the likelihood of the model responding in English, regardless of the query's original language.
    2. gap of current research: Current VLMs show a critical bias towards English in multimodal contexts, limiting their effectiveness in non-English language environments.
    3. innovation: The study employs a dual approach, combining extensive design space ablation with mechanistic analysis of the models' internal representations.
    4. method: The research utilizes a comprehensive methodology including design space exploration and in-depth analysis of model internals to identify the source and nature of the language bias.
    5. contribution: This research contributes to the development of more inclusive VLMs that can better serve non-English contexts, addressing a critical gap in current VLM capabilities and paving the way for more linguistically diverse multimodal AI systems.